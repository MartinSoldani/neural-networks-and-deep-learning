{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MartinSoldani/neural-networks-and-deep-learning/blob/master/NEW_COMP4329_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define shared project folder\n",
        "project_path = '/content/drive/MyDrive/A1 Deep Learning Shared'\n",
        "# Define the data directory path\n",
        "data_path = os.path.join(project_path, 'data')\n",
        "\n",
        "# Check if the shared folder exists\n",
        "if os.path.exists(project_path):\n",
        "    print(f\"âœ… Shared folder is found: {project_path}\")\n",
        "    print(\"ðŸ“ Contents:\")\n",
        "    print(os.listdir(project_path))\n",
        "else:\n",
        "    print(f\"âŒ Shared folder not found at: {project_path}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJ69Q46sn2JP",
        "outputId": "f31a2f88-3959-4d8f-928c-ef2dcbd0058f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "âœ… Shared folder is found: /content/drive/MyDrive/A1 Deep Learning Shared\n",
            "ðŸ“ Contents:\n",
            "['data', '.ipynb_checkpoints', 'NEW COMP4329 Code.ipynb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zvMkj2khDdB6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = np.load(os.path.join(data_path, 'test_data.npy'))\n",
        "test_label = np.load(os.path.join(data_path, 'test_label.npy'))\n",
        "train_data = np.load(os.path.join(data_path, 'train_data.npy'))\n",
        "train_label = np.load(os.path.join(data_path, 'train_label.npy'))"
      ],
      "metadata": {
        "id": "w9JPfEc4ZBZW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Activation(object):\n",
        "    def __tanh(self, x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def __tanh_deriv(self, a):\n",
        "        return 1.0 - a**2\n",
        "\n",
        "    def __logistic(self, x):\n",
        "        return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "    def __logistic_deriv(self, a):\n",
        "        return a * (1 - a)\n",
        "\n",
        "    def __relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def __relu_deriv(self, a):\n",
        "        return 1 * (a > 0)\n",
        "\n",
        "    def __init__(self, activation='tanh'):\n",
        "        if activation == 'logistic':\n",
        "            self.f = self.__logistic\n",
        "            self.f_deriv = self.__logistic_deriv\n",
        "\n",
        "        elif activation == 'tanh':\n",
        "            self.f = self.__tanh\n",
        "            self.f_deriv = self.__tanh_deriv\n",
        "\n",
        "        elif activation == 'relu':\n",
        "            self.f = self.__relu\n",
        "            self.f_deriv = self.__relu_deriv"
      ],
      "metadata": {
        "id": "0MU8faZKaAct"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HiddenLayer(object):\n",
        "    def __init__(self,\n",
        "                 num_inputs,\n",
        "                 num_neurons,\n",
        "                 activation_last_layer,\n",
        "                 activation,\n",
        "                 weights=None,\n",
        "                 bias=None):\n",
        "        \"\"\"\n",
        "        Typical hidden layer of a MLP: units are fully-connected and have\n",
        "        sigmoidal activation function. Weight matrix W is of shape (num_inputs,num_neurons)\n",
        "        and the bias vector b is of shape (num_neurons,).\n",
        "\n",
        "        NOTE : The nonlinearity used here is tanh\n",
        "\n",
        "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
        "\n",
        "        :type num_inputs: int\n",
        "        :param num_inputs: dimensionality of input\n",
        "\n",
        "        :type num_neurons: int\n",
        "        :param num_neurons: number of neurons (hidden units) in this layer\n",
        "\n",
        "        :type activation: string\n",
        "        :param activation: Non linearity to be applied in the hidden\n",
        "                           layer\n",
        "        \"\"\"\n",
        "        self.input = None\n",
        "        self.activation = Activation(activation).f\n",
        "\n",
        "        self.activation_deriv = None\n",
        "        if activation_last_layer:\n",
        "            self.activation_deriv = Activation(activation_last_layer).f_deriv\n",
        "\n",
        "        # this range is based on a popular method (Xavier/Glorot init)\n",
        "        # that helps prevent exploding/vanishing gradients\n",
        "        self.weights = np.random.uniform(\n",
        "                low=-np.sqrt(6. / (num_inputs + num_neurons)),\n",
        "                high=np.sqrt(6. / (num_inputs + num_neurons)),\n",
        "                size=(num_inputs, num_neurons)\n",
        "        )\n",
        "\n",
        "        # if activation == 'logistic':\n",
        "        #     self.W *= 4\n",
        "\n",
        "        self.bias = np.zeros(num_neurons,)\n",
        "\n",
        "        self.grad_W = np.zeros(self.weights.shape)\n",
        "        self.grad_b = np.zeros(self.bias.shape)\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        '''\n",
        "        :type input: numpy.array\n",
        "        :param input: a symbolic tensor of shape (num_inputs,)\n",
        "        '''\n",
        "        z = np.dot(input, self.weights) + self.bias\n",
        "        self.output = (\n",
        "            z if self.activation is None\n",
        "            else self.activation(z)\n",
        "        )\n",
        "\n",
        "        # We save the input so that it can be used later during the backward pass\n",
        "        # FORMULA BP4\n",
        "        # To compute the gradient of the loss with respect to the weights we need:\n",
        "        #   The original input to the layer\n",
        "        self.input = input\n",
        "        #   The error signal (delta) coming from the next layer\n",
        "\n",
        "        # dropout\n",
        "        # shape_output = self.output.shape\n",
        "        # rd_indices = np.random.randint(0, shape_output[0], shape_output[0] * 0.6)\n",
        "        # self.output[rd_indices] *= 0.0\n",
        "        # self.output = self.output/(1- 0.6)\n",
        "        return self.output\n",
        "\n",
        "    # this does only compute it for the hidden layers, not the output layer\n",
        "    # output layer has already been computed in MLP class\n",
        "    def backward(self, delta, output_layer=False):\n",
        "        # compute BP4\n",
        "        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta))\n",
        "        # compute BP3\n",
        "        self.grad_b = delta\n",
        "        # Output layer skips this extra derivative,\n",
        "        # because it was already applied in the loss function\n",
        "        if not output_layer and self.activation_deriv:\n",
        "            # compute BP2\n",
        "            delta = delta.dot(self.weights.T) * self.activation_deriv(self.input)\n",
        "\n",
        "        return delta"
      ],
      "metadata": {
        "id": "1PKQmwqQgxgY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    def __init__(self, layers, activation=[None,'tanh','tanh']):\n",
        "        \"\"\"\n",
        "        :param layers: A list containing the number of units in each layer.\n",
        "        Should be at least two values\n",
        "        :param activation: The activation function to be used. Can be\n",
        "        \"logistic\" or \"tanh\"\n",
        "        \"\"\"\n",
        "        ### initialize layers\n",
        "        self.layers = []\n",
        "        self.params = []\n",
        "\n",
        "        self.activation = activation\n",
        "        for i in range(len(layers) - 1):\n",
        "            self.layers.append(HiddenLayer(layers[i], layers[i+1], activation[i], activation[i+1]))\n",
        "\n",
        "    # forward progress: pass the information through the layers and out the results of final output layer\n",
        "    def forward(self, input):\n",
        "        for layer in self.layers:\n",
        "            output = layer.forward(input)\n",
        "            input = output\n",
        "        return output\n",
        "\n",
        "    # define the objection/loss function, we use mean square error (MSE) as the loss\n",
        "    # you can try other loss, such as cross entropy.\n",
        "    # when you try to change the loss, you should also consider the backward formula for the new loss as well!\n",
        "    def criterion_MSE(self, y, y_hat):\n",
        "        # get activation derivative of last layer\n",
        "        activation_deriv = Activation(self.activation[-1]).f_deriv\n",
        "        # MSE FOR JUST ONE EXAMPLE AT A TIME (NO SUMMATION WHATSOEVER)\n",
        "        error = y - y_hat\n",
        "        loss = np.mean(error**2)\n",
        "        # calculate the MSE's delta of the output layer 2 * (z1 - t)\n",
        "        # FORMULA BP1 IN BOOK\n",
        "        delta = (-2 * error) * activation_deriv(y_hat) # (z1 - t)^2 -> 2 (z1 - t) * act_deriv(z1)\n",
        "        # return loss and delta\n",
        "        return loss , delta\n",
        "\n",
        "    # backward progress\n",
        "    def backward(self, delta):\n",
        "        delta = self.layers[-1].backward(delta, output_layer=True)\n",
        "        for layer in reversed(self.layers[:-1]):\n",
        "            delta = layer.backward(delta)\n",
        "\n",
        "    # update the network weights after backward.\n",
        "    # make sure you run the backward function before the update function!\n",
        "    def update(self, lr):\n",
        "        for layer in self.layers:\n",
        "            layer.weights -= lr * layer.grad_W\n",
        "            layer.bias -= lr * layer.grad_b\n",
        "\n",
        "    # define the training function\n",
        "    # it will return all losses within the whole training process.\n",
        "    def fit(self, input, target, learning_rate=0.1, epochs=100):\n",
        "        \"\"\"\n",
        "        Online learning.\n",
        "        :param input: Input data or features\n",
        "        :param target: Input targets\n",
        "        :param learning_rate: parameters defining the speed of learning\n",
        "        :param epochs: number of times the dataset is presented to the network for learning\n",
        "        \"\"\"\n",
        "        input = np.array(input)\n",
        "        target = np.array(target)\n",
        "        to_return = np.zeros(epochs)\n",
        "\n",
        "        for k in range(epochs):\n",
        "            loss = np.zeros(input.shape[0])\n",
        "\n",
        "            for it in range(input.shape[0]):\n",
        "                i = np.random.randint(input.shape[0])\n",
        "\n",
        "                # forward pass\n",
        "                y_hat = self.forward(input[i])\n",
        "\n",
        "                # backward pass\n",
        "                # calculate BP1 here: cost of last layer (output layer)\n",
        "                result_loss, delta = self.criterion_MSE(target[i], y_hat)\n",
        "                loss[it] = result_loss\n",
        "                self.backward(delta)\n",
        "\n",
        "\n",
        "                # update\n",
        "                self.update(learning_rate)\n",
        "\n",
        "            to_return[k] = np.mean(loss)\n",
        "        return to_return\n",
        "\n",
        "    # define the prediction function\n",
        "    # we can use predict function to predict the results of new data, by using the well-trained network.\n",
        "    def predict(self, x):\n",
        "        x = np.array(x)\n",
        "        output = np.zeros(x.shape[0])\n",
        "        for i in np.arange(x.shape[0]):\n",
        "            output[i] = self.forward(x[i,:])\n",
        "        return output"
      ],
      "metadata": {
        "id": "RykQ-FjHgzuj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "LAYER_NEURONS = [128, 3, 2, 10]\n",
        "LAYER_ACTIVATION_FUNCS = [None, 'relu', 'relu', 'relu']\n",
        "\n",
        "nn = MLP(LAYER_NEURONS, LAYER_ACTIVATION_FUNCS)\n",
        "\n",
        "t0 = time.time()\n",
        "trial1 = nn.fit(train_data, train_label)\n",
        "t1 = time.time()\n",
        "\n",
        "print(f\"============= Model Build Done =============\")\n",
        "print(f\"Time taken to build model: {round(t1 - t0, 4)} seconds.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NP7kTNUzmidz",
        "outputId": "a1af1a3e-ec17-48f3-90ed-daae8bfbefbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============= Model Build Done =============\n",
            "Time taken to build model: 262.7205 seconds.\n"
          ]
        }
      ]
    }
  ]
}